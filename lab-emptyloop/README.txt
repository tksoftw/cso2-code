Explanation: I selected a threshold of 2100 nanoseconds because it effectively separated the baseline loop overhead (i.e. the "main bar" + some noise on the base graph) from the much slower spikes caused by the asynchronous exceptions. I verified my choice by pinning the empty-time program to Core 1 and collecting 10,000 samples. I then checked output of interrupts-difference.py and verified that 10,080 interrupts occurred on Core 1, representing only a 0.8% error between the samples I measured and the actual hardware counts. Finally, I ran the program again while encouraging a context switch by using three long loops on Core 1, as suggested by the lab writeup. I ended up with over 20,000 interrupts despite only measuring 10,000, as well as a large, visible bar around the 10^7.25 nanosecond range.

Context Switching Estimate: I included two graphs (along with the baseline no threshold graph) to demonstrate the difference. All plots use a log-log scale.
In my original run (new-output-graph-2100-no-CS.png), the test with the 2100 threshold, the interrupt count almost perfectly matched the sample count. I believe this is because the program was not context-switched away for any significant period. 
However, in the run where I forced the CPU core to execute other instructions alongside the measurement program with the same 2100 threshold (new-output-graph-2100-CS.png), the massive gaps between interrupt times along with the count of interrupts vs samples measured show that the program was clearly context-switched away.
Specifically, the high-latency outliers in this run all happened around the 10^7.25 nanosecond range. I estimate that this means each major context switch lasted approximately 10^7.25 nanoseconds, or around roughly 17.8ms. 
Additionally, since the total interrupt count was just over 20,000, about double the measurement count of 10,000, I believe this means that the OS scheduler paused my program for about 50% of the experiment time to alternate between background loops.